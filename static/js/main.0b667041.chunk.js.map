{"version":3,"sources":["assert/sound.m4a","App.js","reportWebVitals.js","index.js"],"names":["sound","Howl","src","soundURL","mobilenetModule","require","knnClassifier","App","NOT_TOUCH_LABEL","TOUCHED_LABEL","video","useRef","canPlaySound","classifier","mobilenet","useState","touched","setTouched","init","a","console","log","setupCamera","current","create","load","Promise","resolve","reject","navigator","getUserMedia","webkitGetUserMedia","mozGetUserMedia","msGetUserMedia","stream","srcObject","addEventListener","error","useEffect","on","train","label","i","parseInt","training","embedding","infer","addExample","sleep","run","predictClass","result","confidences","play","ms","setTimeout","className","ref","autoPlay","onClick","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"4ZAAe,MAA0B,kC,wBCOnCA,EAAQ,IAAIC,OAAK,CACrBC,IAAK,CAACC,KAIFC,EAAkBC,EAAQ,KAC1BC,EAAgBD,EAAQ,KA4HfE,MA1Hf,WACE,IAAMC,EAAkB,YAClBC,EAAgB,UAGhBC,EAAQC,mBACRC,EAAeD,kBAAO,GACtBE,EAAaF,mBACbG,EAAYH,mBAClB,EAA8BI,oBAAS,GAAvC,mBAAOC,EAAP,KAAgBC,EAAhB,KAEMC,EAAI,uCAAG,sBAAAC,EAAA,6DACXC,QAAQC,IAAI,YADD,SAELC,IAFK,cAGXF,QAAQC,IAAI,oCAEZR,EAAWU,QAAUjB,EAAckB,SALxB,SAMepB,EAAgBqB,OAN/B,OAMXX,EAAUS,QANC,OAQXH,QAAQC,IAAI,iBACZD,QAAQC,IAAI,sEATD,4CAAH,qDAYJC,EAAc,WAClB,OAAO,IAAII,SAAQ,SAACC,EAASC,GAC3BC,UAAUC,aAAeD,UAAUC,cACjCD,UAAUE,oBACVF,UAAUG,iBACVH,UAAUI,eAETJ,UAAUC,cACXD,UAAUC,aACR,CAACpB,OAAO,IACR,SAAAwB,GACExB,EAAMa,QAAQY,UAAYD,EAC1BxB,EAAMa,QAAQa,iBAAiB,aAAcT,MAE/C,SAAAU,GAAK,OAAIT,EAAO,gDAMxBU,qBAAU,WAOR,OANApB,IAEAlB,EAAMuC,GAAG,OAAO,WACd3B,EAAaW,SAAU,KAGlB,eAGP,IAEF,IAAMiB,EAAK,uCAAG,WAAMC,GAAN,eAAAtB,EAAA,sDACZC,QAAQC,IAAR,WAAgBoB,EAAhB,0BACQC,EAAE,EAFE,YAECA,EArDM,IAmDP,uBAGVtB,QAAQC,IAAR,mBAAwBsB,UAAUD,EAAE,GAtDnB,GAsDuC,KAAxD,MAHU,SAIJE,EAASH,GAJL,OAEiBC,IAFjB,sBAMZtB,QAAQC,IAAI,0BACToB,IAAUhC,EAAcW,QAAQC,IAAI,uEAClCD,QAAQC,IAAI,uCARL,4CAAH,sDAWLuB,EAAW,SAAAH,GACf,OAAO,IAAIf,QAAJ,uCAAY,WAAMC,GAAN,eAAAR,EAAA,6DACX0B,EAAY/B,EAAUS,QAAQuB,MAClCpC,EAAMa,SACN,GAEFV,EAAWU,QAAQwB,WAAWF,EAAWJ,GALxB,SAMXO,EAAM,KANK,OAOjBrB,IAPiB,2CAAZ,wDAWHsB,EAAG,uCAAG,8BAAA9B,EAAA,6DACJ0B,EAAY/B,EAAUS,QAAQuB,MAClCpC,EAAMa,SACN,GAHQ,SAKWV,EAAWU,QAAQ2B,aAAaL,GAL3C,QAKJM,EALI,QAMAV,QAAUhC,GAChB0C,EAAOC,YAAYD,EAAOV,OAlFP,IAmFjBrB,QAAQC,IAAIZ,GACTG,EAAaW,UACdX,EAAaW,SAAU,EACvBvB,EAAMqD,QAERpC,GAAW,KAEfG,QAAQC,IAAIb,GACZS,GAAW,IAEbgC,IAlBU,2CAAH,qDAqBHD,EAAQ,WAAa,IAAZM,EAAW,uDAAN,EAClB,OAAO,IAAI5B,SAAQ,SAAAC,GAAO,OAAI4B,WAAW5B,EAAS2B,OAGpD,OACE,sBAAKE,UAAS,cAASxC,GAAW,WAAlC,UACE,oDACA,uBACEyC,IAAK/C,EACL8C,UAAU,QACVE,UAAQ,IAEV,mEACA,mEACA,sBAAKF,UAAU,UAAf,UACE,yBAAQA,UAAU,MAAMG,QAAS,kBAAMnB,EAAMhC,IAA7C,mBAAsEA,KACtE,yBAAQgD,UAAU,MAAMG,QAAS,kBAAMnB,EAAM/B,IAA7C,mBAAoEA,KACpE,wBAAQ+C,UAAU,MAAMG,QAAS,kBAAMV,KAAvC,wBCvHOW,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,M","file":"static/js/main.0b667041.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/sound.0b763664.m4a\";","import './App.css';\nimport { Howl } from 'howler';\nimport soundURL from './assert/sound.m4a';\nimport { useEffect, useRef, useState } from 'react';\nimport '@tensorflow/tfjs-backend-cpu';\nimport * as tf from '@tensorflow/tfjs';\n\nconst sound = new Howl({\n  src: [soundURL]\n});\n\n\nconst mobilenetModule = require('@tensorflow-models/mobilenet');\nconst knnClassifier = require('@tensorflow-models/knn-classifier');\n\nfunction App() {\n  const NOT_TOUCH_LABEL = 'Not Touch';\n  const TOUCHED_LABEL = 'Touched';\n  const TOUCH_CONFIDENCE = 0.7;\n  const TrainingTime = 50;\n  const video = useRef();\n  const canPlaySound = useRef(true);\n  const classifier = useRef();\n  const mobilenet = useRef();\n  const [touched, setTouched] = useState(false);\n\n  const init = async () => {\n    console.log('intit...');\n    await setupCamera()\n    console.log('setup camera success! Waiting...');\n\n    classifier.current = knnClassifier.create();\n    mobilenet.current = await mobilenetModule.load();\n\n    console.log('Process Done!');\n    console.log('Dont push your hands on your face and click Train Not Touch button');\n  }\n\n  const setupCamera = () => {\n    return new Promise((resolve, reject) => {\n      navigator.getUserMedia = navigator.getUserMedia ||\n        navigator.webkitGetUserMedia ||\n        navigator.mozGetUserMedia || \n        navigator.msGetUserMedia;\n\n      if(navigator.getUserMedia){\n        navigator.getUserMedia(\n          {video: true},\n          stream => {\n            video.current.srcObject = stream;\n            video.current.addEventListener('loadeddata', resolve);\n          },\n          error => reject('Access Your Camera And Reload Website')\n        )\n      }\n    })\n  }\n\n  useEffect(() => {\n    init();\n\n    sound.on('end', function(){\n      canPlaySound.current = true;\n    });\n\n    return () => {\n\n    }\n  },[])\n\n  const train = async label => {\n    console.log(`[${label}] is being Trained...`);\n    for(let i=0; i<TrainingTime; i++){\n      console.log(`Progress ${parseInt((i+1) / TrainingTime * 100)}%`);\n      await training(label)\n    }\n    console.log('Training Successfully!');\n    if(label !== TOUCHED_LABEL)console.log('Slowly bring your hands to your face and click Train Touched button');\n    else console.log('click Run button and Relax =)))))))');\n  }\n\n  const training = label => {\n    return new Promise(async resolve => {\n      const embedding = mobilenet.current.infer(\n        video.current,\n        true\n      );\n      classifier.current.addExample(embedding, label);\n      await sleep(100);\n      resolve();\n    })\n  }\n\n  const run = async () => {\n    const embedding = mobilenet.current.infer(\n      video.current,\n      true\n    );\n    const result = await classifier.current.predictClass(embedding);\n    if(result.label === TOUCHED_LABEL &&\n        result.confidences[result.label] > TOUCH_CONFIDENCE){\n          console.log(TOUCHED_LABEL);\n          if(canPlaySound.current){\n            canPlaySound.current = false;\n            sound.play();\n          }\n          setTouched(true);\n    }else{\n      console.log(NOT_TOUCH_LABEL);\n      setTouched(false);\n    }\n    run()\n  }\n\n  const sleep = (ms = 0) => {\n    return new Promise(resolve => setTimeout(resolve, ms))\n  }\n\n  return (\n    <div className={`App ${touched && 'touched'}`}>\n      <h1>ACCESS YOUR CAMERA</h1>\n      <video \n        ref={video}\n        className='video'\n        autoPlay\n      />\n      <h1>Open Your Console To Get Tutorial</h1>\n      <h2>(Ctrl + Shift + I -> tab Console)</h2>\n      <div className='control'>\n        <button className='btn' onClick={() => train(NOT_TOUCH_LABEL)}>Train {NOT_TOUCH_LABEL}</button>\n        <button className='btn' onClick={() => train(TOUCHED_LABEL)}>Train {TOUCHED_LABEL}</button>\n        <button className='btn' onClick={() => run()}>Run</button>\n      </div>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}